# 13주차 정리노트

상태: 시작 전

## 201904197김유성

### 합성곱 층과 풀링 층이 자세하게 무엇인가?

합성곱 층(Convolutional Layer)은 이미지 처리에서 사용되는 층으로, 입력 데이터에 대해 필터와의 합성곱 연산을 수행하여 특징 맵을 생성한다. 이를 통해 이미지의 특징을 추출하고 공간적인 구조를 파악할 수 있다.

풀링 층(Pooling Layer)은 합성곱 층의 출력인 특징 맵을 다운샘플링하여 크기를 줄이는 역할을 한다. 대표적으로 최대 풀링(Max Pooling)과 평균 풀링(Average Pooling)이 있으며, 이를 통해 특징 맵의 크기를 감소시키고 계산량을 줄일 수 있다.

합성곱 층(Convolutional Layer)은 예를 들어 이미지 인식을 위한 신경망을 구성한다고 가정. 입력 이미지에 대해 합성곱 층은 작은 필터를 적용하여 이미지의 특징을 추출한다. 예를 들어, 강아지를 인식하는 신경망에서 합성곱 층은 강아지의 귀, 코, 눈 등과 같은 특징들을 감지할 수 있다. 이러한 특징들은 여러 개의 필터가 동시에 작동하여 특징 맵을 생성하게 된다.

풀링 층(Pooling Layer)은 예를 들어 합성곱 층에서 생성된 특징 맵의 크기를 줄이는 역할을 한다. 최대 풀링(Max Pooling)은 특징 맵에서 가장 큰 값만 선택하여 다운샘플링한다. 이를 통해 이미지의 위치 변화에 상대적으로 덜 민감해지며, 중요한 특징을 보존하면서 크기를 줄일 수 있다. 예를 들어, 강아지의 얼굴을 인식하는 신경망에서, 최대 풀링은 강아지의 눈, 코, 입 등 중 가장 강조되는 특징을 선택하고, 불필요한 세부 정보를 감소시킨다.

이와 같이, 합성곱 층과 풀링 층은 이미지 처리에서 중요한 역할을 한다. 합성곱 층은 이미지의 특징을 추출하고, 풀링 층은 특징 맵의 크기를 줄여 계산량을 줄이고 중요한 정보를 보존한다.

### 평균 풀링 층,전역 평균 풀링 층,최대/평균을 계산하는 풀링층에 대한 자세한 내용을 알고싶다.

### <평균 풀링 층>

평균 풀링 층(Average Pooling Layer)은 특징 맵에서 영역의 평균 값을 계산하여 다운샘플링하는 역할을 한다. 이를 통해 특징 맵의 크기를 줄이고 계산량을 감소시킬 수 있다. 평균 풀링은 영역의 평균 값을 사용하기 때문에 최대 풀링에 비해 더 많은 정보를 보존한다. 평균 풀링은 주로 이미지 분류 작업에서 사용되며, 이미지의 전반적인 특징을 보존하면서 이미지의 세부 사항을 상대적으로 줄이는 데 적합하다.

### <평균 풀링 층>

평균 풀링 층(Average Pooling Layer)은 특징 맵에서 영역의 평균 값을 계산하여 다운샘플링하는 역할을 한다. 이를 통해 특징 맵의 크기를 줄이고 계산량을 감소시킬 수 있다. 평균 풀링은 영역의 평균 값을 사용하여 특징 맵을 다운샘플링한다. 이 방법은 최대 풀링에 비해 더 많은 정보를 보존하며, 이미지 분류 작업에서 주로 사용된다. 평균 풀링은 이미지의 전반적인 특징을 보존하면서 이미지의 세부 사항을 상대적으로 줄이는 데 적합하다.

### <전역 평균 풀링 층>

전역 평균 풀링 층(Global Average Pooling Layer)은 특징 맵의 전체 영역에 대한 평균 값을 계산하여 다운샘플링한다. 이 방법은 특징 맵의 크기를 크게 줄이며, 적은 수의 파라미터로 전체 영역에 대한 정보를 요약한다. 전역 평균 풀링은 주로 마지막 합성곱 층 뒤에 사용되며, 분류 작업에서 클래스별 확률을 계산하기 위해 사용된다. 전역 평균 풀링은 네트워크의 전체적인 구조를 간단하게 만들어 파라미터의 수를 줄이는 장점이 있다.

### VGGNet에 특징이 무엇인가?

VGGNet은 2014년에 개발된 이미지 인식을 위한 합성곱 신경망 구조이다. 이 네트워크는 깊은 구조로 알려져 있으며, 작은 크기의 커널을 사용하여 깊은 합성곱 층과 풀링 층으로 구성되어 있다.

VGGNet의 핵심 아이디어는 작은 필터를 연속적으로 쌓아 깊은 신경망을 구성하는 것이다. 특히, VGGNet은 3x3 크기의 작은 필터를 사용하여 2개의 3x3 합성곱 층을 연속적으로 쌓은 구조를 가지고 있다. 이렇게 작은 필터를 사용하면 신경망이 더 깊어지면서 특징을 더 정교하게 추출할 수 있다.

VGGNet은 합성곱 층과 풀링 층의 연속적인 구조를 반복하여 신경망을 깊게 만든다. 이러한 구조는 일반적으로 16개 또는 19개의 합성곱 층으로 구성된다. 이러한 깊은 구조는 다양한 이미지 인식 작업에서 우수한 성능을 보여준다.

VGGNet의 장점 중 하나는 간단하면서도 효과적인 구조를 가지고 있다는 점이다. 작은 필터를 사용하고 합성곱 층과 풀링 층의 연속적인 구조를 반복함으로써 네트워크를 간결하고 효율적으로 만들 수 있습니다. 이러한 구조는 많은 컴퓨터 비전 태스크에 적용될 수 있다.

하지만 VGGNet은 깊은 구조를 가지면서 많은 계산 비용이 필요하다는 단점도 있다. 특히 학습 및 추론 단계에서 많은 계산 리소스가 필요할 수 있다.

### ResNet (Residual Network)의 특징이 무엇인가?

ResNet은 2015년에 소개된 심층 신경망 구조로, 컴퓨터 비전 분야에서 매우 성공적으로 적용되었다. ResNet의 주요 아이디어는 스킵 연결(skip connection)이라는 개념을 도입한 것이다.

스킵 연결은 네트워크의 일부를 건너뛰어 입력과 출력을 직접 연결하는 것을 의미한다. 이렇게 스킵 연결을 사용하면 네트워크가 더 깊어지더라도 그레디언트 소실(gradient vanishing) 문제를 완화할 수 있다. 그레디언트 소실 문제는 깊은 신경망에서 그레디언트가 너무 작아져 역전파 과정에서 제대로 전달되지 않는 문제다.

ResNet은 기본적으로 잔차 블록(residual block)이라는 구조로 이루어져 있습니다. 잔차 블록은 입력과 출력을 직접 연결한 스킵 연결과 함께 합성곱 층을 포함하고 있다.

ResNet은 많은 변종이 존재하며, 대표적인 예로는 ResNet-50, ResNet-101, ResNet-152 등이 있다. 이러한 변종들은 서로 다른 깊이와 파라미터 수를 가지고 있으며, 특정한 컴퓨터 비전 태스크에 맞게 선택할 수 있다.

ResNet은 이미지 인식, 객체 검출, 세분화 등 다양한 컴퓨터 비전 태스크에서 우수한 성능을 보여주었다. 더불어, ResNet은 다른 신경망 구조에 비해 상대적으로 적은 계산 비용을 요구하고, 높은 정확도를 제공하는 장점을 가지고 있다.

따라서, ResNet은 깊은 신경망에서 그레디언트 소실 문제를 해결하고, 높은 성능과 효율성을 동시에 제공하는 신경망 구조로 널리 사용되고 있다.