# 9주차 정리노트

상태: 시작 전

## 유성

# 앙상블

앙상블은 일련의 예측기로부터 예측을 수집하여 최종 예측을 수행하는 방법입니다. 앙상블 학습은 예측기들의 결과를 종합하여 예측값을 지정하는 학습 방법.

## 앙상블 방법

- 투표기반 분류기: 앙상블에 포함된 예측기들의 예측값을 다수결 투표 또는 확률 예측값의 평균으로 결정
- 배깅과 페이스팅: 동일한 예측기를 훈련 세트의 다양한 부분집합에 학습시키는 방식으로, 각 예측기의 결과를 평균하여 최종 예측값을 결정
- 랜덤 포레스트: 결정 트리의 앙상블을 최적화한 모델로, 트리의 분할에 무작위성을 주입하여 편향을 손해보는 대신 분산을 낮춤
- 부스팅: 약한 학습기들을 연결하여 강한 성능의 학습기를 만드는 앙상블 기법으로, 순차적으로 이전 학습기의 결과를 바탕으로 성능을 향상

앙상블은 다양한 예측기들을 결합하여 더욱 강력한 예측 성능을 제공

## 투표기반 분류기

투표기반 분류기는 앙상블에 포함된 예측기들의 예측값을 다수결 투표 또는 확률 예측값의 평균으로 결정하는 앙상블 방법입니다. 직접 투표 방식과 간접 투표 방식이 있음.

직접 투표 방식은 다수결 투표를 통해 예측값을 결정하고, 간접 투표 방식은 예측기들의 예측한 확률값들의 평균값으로 예측값을 결정합니다. 간접 투표 방식이 직접 투표 방식보다 성능이 좋을 수 있음.

 투표기반 분류기는 앙상블에 포함된 분류기들 사이의 독립성이 전제되는 경우 개별 분류기보다 정확한 예측이 가능하며, 독립성이 보장되지 못한다면 성능이 낮아질 수 있음

## 배깅과 페이스팅

배깅과 페이스팅은 동일한 예측기를 훈련 세트의 다양한 부분집합에 학습시키는 앙상블 학습 방법.

배깅은 중복을 허용한 샘플링 방식으로, 여러 번 중복해서 샘플링된 부분집합에 대해 예측기를 학습페이스팅은 중복을 허용하지 않는 샘플링 방식으로, 한 번만 샘플링된 부분집합에 대해 예측기를 학습시킴.

 배깅과 페이스팅은 각 예측기의 결과를 평균하여 최종 예측값을 결정합니다. 이 방법은 과대적합의 위험성을 줄이고 예측 성능을 향상시킬 수 있음.

## 랜덤 포레스트

랜덤 포레스트는 배깅 방식을 적용한 결정 트리의 앙상블을 최적화한 모델입니다. 랜덤 포레스트는 결정 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신에 무작위성을 주입하여 편향을 손해보는 대신 분산을 낮춤.이를 통해 다양한 예측기를 생성하고 그 결과를 평균하여 최종 예측값을 결정합니다. 랜덤 포레스트는 과대적합을 피하고 일반화 성능을 높이는 데 효과적.

# 부스팅

부스팅은 약한 학습기들을 연결하여 강한 성능의 학습기를 만드는 앙상블 기법입니다. 부스팅은 순차적으로 이전 학습기의 결과를 바탕으로 성능을 향상시킵니다.

## 부스팅 방법

1. 에이다부스트(AdaBoost): 에이다부스트는 이전 학습기의 예측이 틀린 샘플에 가중치를 높여서 다음 학습기가 더 집중적으로 학습하도록 하는 방식.이전 학습기들의 오차를 고려하여 가중치를 업데이트하고, 예측기의 가중치를 결합하여 최종 예측을 수행.
2. 그레디언트 부스팅(Gradient Boosting): 그레디언트 부스팅은 이전 학습기의 예측값과 실제값의 차이인 잔차(residual)를 다음 학습기가 학습하도록 하는 방식. 이전 학습기에 의한 예측값과 실제값의 차이를 줄이는 방향으로 새로운 학습기를 학습시키고, 모든 예측기의 결과를 결합하여 최종 예측을 수행.
3. XGBoost: XGBoost는 그레디언트 부스팅을 기반으로 한 강력한 부스팅 알고리즘입니다. XGBoost는 그레디언트 부스팅의 단점을 보완하고 성능을 향상시키기 위해 다양한 기술과 최적화 방법을 적용. XGBoost는 트리 기반 학습 알고리즘으로 구성되어 있으며, 높은 예측 성능과 빠른 학습 속도를 제공.
4. LightGBM: LightGBM은 Microsoft에서 개발한 분산 그레디언트 부스팅 프레임워크입니다. LightGBM은 대용량 데이터셋에 대한 학습과 예측을 빠르게 처리할 수 있는 기능을 제공. LightGBM은 트리 분할에 대한 최적화 알고리즘을 사용하여 학습 속도와 성능을 향상시키는 효과적인 기법을 적용.

## 부스팅의 장점

- 약한 학습기들을 결합하여 강력한 예측 성능을 제공
- 과소적합된 모델을 보완하고 일반화 성능을 높이는 데 효과적
- 특징 선택, 특징 중요도 추정 등 다양한 분석에 활용할 수 있음.

# 부스팅의 주의사항

- 부스팅은 강력한 모델을 만들 수 있지만, 학습 데이터에 과적합될 수 있습니다. 따라서 적절한 규제와 조기 종료(Early Stopping) 등을 통해 과적합을 방지해야 함.
- 학습에 사용되는 데이터에 노이즈나 이상치가 있는 경우, 부스팅 알고리즘이 이를 과도하게 학습할 수 있습니다. 이를 방지하기 위해 데이터 전처리를 신중하게 수행해야 함.
- 부스팅은 학습 시간이 오래 걸릴 수 있으며, 모델의 복잡성으로 인해 메모리 사용량이 크게 증가할 수 있습니다. 이를 고려하여 적절한 하드웨어 리소스를 할당해야 함.
- 부스팅 알고리즘의 하이퍼파라미터 설정에 따라 성능이 크게 달라질 수 있습니다. 하이퍼파라미터 튜닝을 통해 최적의 모델을 찾는 데 시간을 투자해야 함.

# 동현

# 앙상블

- 일련의 예측기로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 할 수 있다
- 여기서 아이디어를 가져와 여러 개의 예측기로 만든 그룹을 **앙상블**이라 함

### 앙상블 학습

- 예측기 여러 개의 결과를 종합하여 예측값을 지정하는 학습

### 앙상블 방법

- 앙상블 학습을 지원하는 앙상블 학습 알고리즘

---

# 1. 투표기반 분류기

- 동일한 훈련 세트에 대해 여러 종류의 분류기 이용한 앙상블 학습 적용 후 직접 또
는 간접 투표를 통해 예측값 결정
- 이 분류기는 투표 방법에 따라서 두가지 방법으로 나뉨

## 1-1 직접 투표

- 앙상블에 포함된 예측기들의 예측값들을 다수결 투표로 결정

## 1-2 간접 투표

- 모든 예측기가 predict_proba() 메서드와 같은 확률 예측 기능을 지원해야한다는 전제를 가짐
- 앙상블에 포함된 예측기들의 예측한 확률값들의 평균값으로 예측값 결정
- **직접투표 방식보다 성능 좀 더 좋음**

### 투표식 분류기 특징

- 앙상블에 포함된 분류기들 사이의 독립성이 전재되는 경우 개별 분류기 보다 정확한 예측 가능
- 독립성이 보장되지 못한다면 투표식 분류기의 성능이 더 낮아질 수 있음

### 큰 수의 법칙

- 반복 시행하는 횟수가 많거나 표본이 커질수록 일정한 수준으로 수렴 되고 비교적 정확한 예측이 가능하다는 의미

# 2. 배깅과 페이스팀

- 동일한 예측기를 훈련 세트의 다양한 부분집합을 대상으로 학습시키는 방식
- 학습에 사용되는 부분집합에 따라 훈련세트가 다른 예측기를 학습시키는 앙상블 학습 기법
- 부분집합의 중복 허용 여부에 따라 학습방식이 달라짐
    - 배깅 - 중복 허용 샘플링 방식
    - 페이스팅 - 중복 미허용 샘플링 방식
- 배깅은 통계 분야에서 부트스트래핑, 즉, 중복허용 리샘플링으로 불림
- 배깅 방식은 동일 샘플을 여러번 샘플링할 수 있음

## 2-1 사이킷런의 배깅과 페이스팅

### 배깅, 페이스팅 예측 방식

- 개별 예측기의 결과를 종합해서 최종 예측값 지정
    - 분류 모델 - 직접 투표 방식 사용. 즉, 수집된 예측값들 중에서 통계적 최빈값 선택
    - 회귀 모델 - 수집된 예측값들의 평균값 선택

### 앙상블 학습의 편향과 분산

- 개별 예측기의 경우에 비해 편향은 비슷하지만 분산은 줄어든다
    - 즉, 과대적합의 위험성이 줄어듬
- 개별 예측기
    - 배깅, 페이스팅 방식으로 학습하면 전체 훈련 세트를 대상으로 학습한 경우에 비해 편향이 커짐. 따라서 과소적합 위험성 커짐.

## 2-2 oob 평가

- 배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링 되고, 어떤 것은 전혀 선택되지 않음.
- oob 샘플을 활용하여 앙상블 학습에 사용된 개별 예측기의 성능 평가 가능
    - 앙상블의 평가는 각 예측기의 oob 평가를 평균하여 얻음
    - oob샘플 - 선택되지 않은 훈련 샘플

# 3. 랜덤 패치와 랜덤 서브스페이스

### BaggingClassifier 는 특성에 대한 샘플링 지원

- max_features 와 bootstrap_features 매개변수로 조절
    - max_features - 학습에 사용할 특성 수 지정, 특성 선택은 무작위
    - bootstrap_features - 학습에 사용할 특성을 선택할 때 중복 허용 여부 지정
- 이미지 등과 같이 매우 높은 차원의 데이터셋을 다룰 때 유용
- 특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춤

### 랜덤 패치 기법

- 훈련 샘플과 훈련 특성 모두를 대상으로 중복을 허용하며, 임의의 샘플 수와 임의의 특성 수만큼을 샘플링해서 학습하는 기법

### 랜덤 서브스페이스 기법

- 전체 훈련 세트를 학습 대상으로 삼지만 훈련 특성은 임의의 특성 수만큼 샘플링해서 학습하는 기법

# 4. 랜덤 포레스트

- 배깅/페이스팅 방법을 적용한 결정트리의 앙상블을 최적화한 모델

### 랜럼 포레스트 알고리즘

- 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위성을 더 주입
- 트리를 더욱 다양하게 만들고, 편향을 손해보는 대신 분산을 낮춤

## 4-1 엑스트라 트리

### 랜덤포레스트의 노드 분할 방식

- 특성 - 무작위 선택
- 특성 임곗값 - 무작위로 분할한 다음 최적값 선택

### 엑스트라 트리의 노드 분할 방식

- 특성과 특성 임곗값 모두 무작위 선택
- 일반적인 랜덤포레스트보다 속도가 훨씬 빠름
    - 편향은 늘고, 분산은 줄어듦

## 4-2 특성 중요도

- 해당 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지를 측정
    - 불순도를 많이 줄이면 그만큼 중요도가 커짐
- 훈련이 끝난 뒤 특성마다 중요도의 전체 합이 1이 되도록 결과값을 정규화 함

# 5. 부스팅

- 성능이 약한 학습기를 여러 개 연결하여 강한 성능의 학습기를 만드는 앙상블 기법
    - 순차적으로 이전 학습기의 결과를 바탕으로 성능을 조금씩 높혀가는 방식
    - 순차적으로 학습하기에 배깅/페이스팅에 비해 확장성이 떨어짐
- 에이다부스트 & 그레이디언트 부스팅 등이 있음

## 5-1 에이다부스트

- 좀 더 나은 예측기를 생성하기 위해 잘못 적용된 가중치를 조정하여 새로운 예측기를 추가하는 앙상블 기법
- 이전 모델이 제대로 학습하지 못한, 즉 과소적합했던 훈련 샘플들에 대한 가중치를 더 높이는 방식으로 새로운 모델 생성
- 새로운 예측기는 학습하기 어려운 샘플에 조금씩 더 잘 적응하는 모델이 연속적으로 만들어져 감

## 5-2 그레이디언트 부스팅

- 이전 학습기에 의한 오차를 보정하도록 새로운 예측기를 순차적으로 추가하는 아이디어는 에이다 부스트와 동일
- 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차 에 대해 새로운 예측기를 학습시킴
    - 잔여 오차 - 예측값과 실제값 사이의 오차
- 잔여 오차를 줄이는 방향으로 모델을 학습시키는데, 이 때 경사 하강법을 사용하여 최적화함

# 6. 스태킹

- 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시켜줌
- 사이킷런은 스태킹을 직접 지원하지 않음
- 블렌더를 학습시키는 일반적인 방법은 홀드 아웃 세트를 사용