# 12주차 정리노트

상태: 시작 전

## 201904197김유성

## 1.케라스 함수형 API 활용의 장점은?

1. **다중 입력 및 출력 모델 지원**: 함수형 API는 여러 개의 입력 및 출력을 가진 복잡한 모델을 생성하는 데 용이함. 이는 다양한 작업에 유용하며, 다중 모달(Multi-modal) 입력 및 다중 출력 모델을 쉽게 구성가능.
2. **공유 레이어**: 함수형 API를 사용하면 여러 층에서 동일한 레이어를 재사용할 수 있다. 이로 인해 모델의 가중치가 공유되며, 모델 파라미터 수를 줄일 수 있다. 예를 들어, 이미지 처리 및 텍스트 처리에서 공유된 임베딩 레이어를 사용할 수 있다.
3. **비순환 그래프**: 함수형 API는 비순환 계산 그래프를 지원힌다. 이는 주로 순환 신경망(RNN)과 같은 복잡한 구조를 가진 모델을 구성할 때 필요하다.
4. **다른 모델로의 분기 및 병합**: 함수형 API를 사용하면 모델의 출력을 다른 모델로 분기하고 병합할 수 있다. 이는 앙상블 모델을 만들거나 다양한 모델 아키텍처를 결합할 때 유용하다.
5. **커스텀 모델 구성**: 함수형 API를 사용하면 사용자 정의 레이어 및 모델을 쉽게 정의하고 통합할 수 있다. 이로써 모델 아키텍처를 보다 유연하게 구성할 수 있다.
6. **디버깅 및 시각화**: 함수형 API는 모델을 각각의 레이어로 분해하고 각 레이어의 출력을 확인하거나 시각화하는 데 용이하다. 이를 통해 모델 구조와 중간 출력을 더 쉽게 이해하고 디버깅할 수 있다.
7. **모델 재사용**: 함수형 API로 정의된 모델의 부분을 쉽게 추출하여 다른 모델에 재사용할 수 있다. 이는 전이 학습(Transfer Learning) 및 모델 파인튜닝에 유용하다.
8. **동적 모델 구성**: 함수형 API를 사용하면 모델의 구조를 동적으로 조작할 수 있다. 이는 반복적인 모델 구성 또는 하이퍼파라미터 조정을 위해 필요한 경우 유용히다.

## 다중레이블 이진분류의 장단점은 무엇인가?

다중 레이블 이진 분류의 장점:

1. **현실적인 모델링**: 다중 레이블 이진 분류는 현실 세계의 복잡한 문제를 모델링하는 데 적합하다. 예를 들어, 문서 분류에서 여러 주제에 동시에 속할 수 있는 경우를 다룰 때 유용하다.
2. **레이블 간 관계 고려**: 다중 레이블 이진 분류는 레이블 간 상호 작용 및 의존성을 고려할 수 있다. 이것은 다중 클래스 분류에서 불가능한 것일 수 있다.
3. **유연성**: 각 레이블에 대한 이진 분류기를 개별적으로 훈련하기 때문에 레이블 간 다양한 속성과 특성을 모델링할 수 있다.
4. **확장성**: 다중 레이블 이진 분류는 다양한 레이블 수에 대해 확장 가능하다. 새로운 레이블을 추가하거나 기존 레이블을 제거하는 데 비교적 쉽게 대응할 수 있다.

다중 레이블 이진 분류의 단점:

1. **데이터 부족 문제**: 다중 레이블 이진 분류 모델은 각 레이블별로 이진 분류를 수행하므로 각 레이블에 대한 데이터가 부족한 경우 모델의 성능이 저하될 수 있다.
2. **모델 복잡성**: 다중 레이블 이진 분류 모델은 여러 개의 이진 분류기로 구성되므로 모델이 복잡해질 수 있다. 이는 과적합(overfitting)의 위험성을 증가시킬 수 있다.
3. **클래스 불균형**: 다중 레이블 이진 분류에서 클래스 간 불균형이 발생할 수 있으며, 이로 인해 일부 레이블에 대한 학습이 어려울 수 있다.
4. **해석의 어려움**: 다중 레이블 이진 분류 모델은 복잡하며 각 레이블에 대한 분류 결정을 해석하기 어려울 수 있다.
5. **계산 리소스**: 다중 레이블 이진 분류 모델은 여러 개의 이진 분류기를 관리하고 훈련해야 하므로 계산 리소스가 더 많이 필요할 수 있다.

## 케라스에 적합한 데이터 분석

1. 이미지 분석 (Computer Vision): 이미지 분류, 객체 감지, 이미지 생성, 스타일 변환 등과 같은 컴퓨터 비전 작업을 수행할 수 있다. 예를 들어, 이미지 분류 모델을 훈련하여 이미지에 어떤 객체가 포함되어 있는지 자동으로 인식하거나, 이미지 생성 모델을 사용하여 새로운 이미지를 생성할 수 있다.
2. 자연어 처리 (Natural Language Processing, NLP): 텍스트 분류, 감정 분석, 기계 번역, 텍스트 생성과 같은 NLP 작업을 수행할 수 있다. 예를 들어, 텍스트 분류 모델을 훈련하여 텍스트 문서를 주제 또는 카테고리에 따라 분류하거나, 기계 번역 모델을 사용하여 다국어 간 텍스트 번역을 자동화할 수 있다.

Keras는 TensorFlow, Theano 등의 백엔드 엔진을 지원하며, 다양한 미리 정의된 신경망 층과 모델을 제공하여 모델 구성을 단순하다. 또한 커스텀 모델 및 레이어를 쉽게 작성하고 확장할 수 있다.

쉬운 구현과 함께 Keras는 딥 러닝 모델의 훈련 및 평가를 위한 다양한 유틸리티 및 편리한 기능을 제공하여 데이터 과학자와 기계 학습 엔지니어가 효율적으로 작업할 수 있도록 도와다.

---

## 1. 텐서보드 말고도 또 어떤 시각화 방법들이 있을까?

- **Weights & Biases**

## 2. 그리드탐색과 랜덤탐색 이외에 어떤 튜닝 방법이 있을까?

하이퍼 파라미터를 튜닝할때는 그리드탐색과 랜덤 탐색등의 방법을 사용할 수 있는데 이외에 방법으로는 베이지안 최적화 방법이 있다.

베이지안 최적화는 하이퍼파라미터 튜닝에 사용되는 효율적인 전역 최적화 방법으로 이 방법은 베이즈 이론을 기반으로 하며, 이전의 결과를 동적으로 사용하여 다음에 탐색할 하이퍼파라미터를 결정한다.

베이지안 최적화 과정은 어떻게 이루어지냐면 초기 단계에서는 일부 하이퍼파라미터를 랜덤하게 선택하거나, 사용자가 정의한 특정 값으로 시작하면서 선택된 하이퍼파라미터로 모델을 학습시키고 성능을 평가한다. 그리고 평가 결과를 바탕으로, 베이지안 방법을 사용하여 '성능을 더욱 향상시킬 수 있을 가능성이 높은' 다음 하이퍼파라미터를 추정하고 이걸 반복하면서 최적의 하이퍼파라미터를 찾아간다.

이전 탐색 결과를 바탕으로 더 나은 하이퍼파라미터를 빠르게 찾아갈 수 있기 때문에 그리드 탐색이나 랜덤 탐색보다 효율적인 경우가 많지만 베이지안 최적화 역시 최적의 결과를 보장하는 것은 아니며, 최적화 과정이 복잡하거나 시간이 오래걸리는 경우도 있다

## 3. 미니배치 경사하강법 보다 더 좋은 성능의 옵티마이저는 뭐가 있을까?

미니배치 경사 하강법보다 더 좋은 성능의 옵티마이저에는 AdaGrad이라는 각 매개변수에 대해 학습률을 개별적으로 조정해주는 최적화 알고리즘이 존재하지만 이 AdaGrad 알고리즘은 학습이 진행됨에 따라 학습률이 과도하게 감소하여 결국 학습이 느려지거나 완전히 멈추는 문제가 있다.

이때 AdaGrad의 학습률 감소 문제를 해결하기 위해 제안된 **RMSProp(Root Mean Square Propagation)**이라는 최적화 알고리즘이 있는데 이 알고리즘은 각 매개변수의 학습률을 독립적으로 조정하여 최적화를 진행하는 방법을 사용하고 이러한 방법은 각 매개변수가 다른 크기의 업데이트를 필요로 하는 상황, 예를 들어 희소한 데이터를 다루는 경우에 유용하다.

RMSProp가 학습률을 각 매개변수에 대해 개별적으로 조정하면서도, AdaGrad처럼 학습률이 너무 빨리 감소하는 문제를 해결할 수 있었던 것은  RMSProp는 각 매개변수에 대한 그래디언트의 제곱을 누적하여 저장하되, 가장 최근의 반복만을 고려하는데 이를 위해 지수 이동 평균을 사용하여 가장 최근의 그래디언트 값에 더 큰 가중치를 부여하고, 이전 그래디언트 값에는 낮은 가중치를 부여했기 때문이다. 이는 각 반복에서 그래디언트의 크기에 따라 학습률을 동적으로 조정함으로써, 빠르고 효율적인 학습을 가능하게 하고 따라서  RMSProp은 다양한 종류의 딥러닝 모델에서 잘 작동하며, 특히 순환 신경망 같은 복잡한 모델에서 주로 사용된다.